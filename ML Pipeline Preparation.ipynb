{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "#import xgboost as xgb\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer, classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "#from sklearn import XGBModel\n",
    "\n",
    "pd.set_option('max_rows', 1000)\n",
    "pd.set_option('max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "#def load_data():\n",
    "engine = create_engine('sqlite:///processed_etl_pipeline_data.db')\n",
    "df = pd.read_sql_table('processed_etl_pipeline_data', engine)\n",
    "\n",
    "#Drop any null records\n",
    "df = df.dropna()\n",
    "\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)\n",
    "#return df, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Purpose:  Normalize and clean message text for modeling by removing stems and endings, standardizing text and formatting\n",
    "    \n",
    "    Input: string (message data)\n",
    "\n",
    "    Output: List of cleaned strings\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize by converting to lowercase and removing punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "    # Stem word tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                    ('vector', CountVectorizer(tokenizer = tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vector', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        str...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.7, test_size = 0.3, random_state = 42)\n",
    "\n",
    "np.random.seed(42)\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(y_test, y_preds, col_names):\n",
    "    \"\"\"\n",
    "    Purpose: Generate performance metrics for model\n",
    "    \n",
    "    Inputs:\n",
    "    y_test:  actual data values\n",
    "    y_preds:  model predicted data values\n",
    "    col_names: list containing all Y category column names for each predicted category\n",
    "    \n",
    "    Returns: dataframe showing actual and predicted above, plus accuracy, precision, recall, and F1\n",
    "    \"\"\"\n",
    "    # Create blank list to hold all outcomes from loop\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Loop to generate stats for each column\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(y_test[:,i], y_preds[:,i], normalize = True)\n",
    "        precision = precision_score(y_test[:,i], y_preds[:,i], average = 'micro')\n",
    "        recall = recall_score(y_test[:,i], y_preds[:,i], average = 'micro')\n",
    "        f1 = f1_score(y_test[:,i], y_preds[:,i], average = 'micro')\n",
    "        \n",
    "        # Append all metrics to blank list\n",
    "        all_metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Convert list to dataframe\n",
    "    performance_df = pd.DataFrame(np.array(all_metrics), index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "    \n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall  F1 Score\n",
      "related                 0.977206   0.977206  0.977206  0.977206\n",
      "request                 0.962291   0.962291  0.962291  0.962291\n",
      "offer                   0.999719   0.999719  0.999719  0.999719\n",
      "aid_related             0.966371   0.966371  0.966371  0.966371\n",
      "medical_help            0.984944   0.984944  0.984944  0.984944\n",
      "medical_products        0.990573   0.990573  0.990573  0.990573\n",
      "search_and_rescue       0.994512   0.994512  0.994512  0.994512\n",
      "security                0.995497   0.995497  0.995497  0.995497\n",
      "military                0.998171   0.998171  0.998171  0.998171\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.980864   0.980864  0.980864  0.980864\n",
      "food                    0.969185   0.969185  0.969185  0.969185\n",
      "shelter                 0.971859   0.971859  0.971859  0.971859\n",
      "clothing                0.997608   0.997608  0.997608  0.997608\n",
      "money                   0.995779   0.995779  0.995779  0.995779\n",
      "missing_people          0.997186   0.997186  0.997186  0.997186\n",
      "refugees                0.995075   0.995075  0.995075  0.995075\n",
      "death                   0.993246   0.993246  0.993246  0.993246\n",
      "other_aid               0.967638   0.967638  0.967638  0.967638\n",
      "infrastructure_related  0.991276   0.991276  0.991276  0.991276\n",
      "transport               0.994512   0.994512  0.994512  0.994512\n",
      "buildings               0.989728   0.989728  0.989728  0.989728\n",
      "electricity             0.998874   0.998874  0.998874  0.998874\n",
      "tools                   0.999578   0.999578  0.999578  0.999578\n",
      "hospitals               0.998452   0.998452  0.998452  0.998452\n",
      "shops                   0.999578   0.999578  0.999578  0.999578\n",
      "aid_centers             0.998171   0.998171  0.998171  0.998171\n",
      "other_infrastructure    0.995216   0.995216  0.995216  0.995216\n",
      "weather_related         0.967356   0.967356  0.967356  0.967356\n",
      "floods                  0.992824   0.992824  0.992824  0.992824\n",
      "storm                   0.989869   0.989869  0.989869  0.989869\n",
      "fire                    0.999296   0.999296  0.999296  0.999296\n",
      "earthquake              0.982130   0.982130  0.982130  0.982130\n",
      "cold                    0.998171   0.998171  0.998171  0.998171\n",
      "other_weather           0.996201   0.996201  0.996201  0.996201\n",
      "direct_report           0.963416   0.963416  0.963416  0.963416\n"
     ]
    }
   ],
   "source": [
    "#Evaluate performance for training set\n",
    "y_train_preds = pipeline.predict(X_train)\n",
    "\n",
    "#Create list of column names for function\n",
    "col_names = list(Y.columns)\n",
    "\n",
    "print(get_performance_metrics(np.array(y_train), y_train_preds, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall  F1 Score\n",
      "related                 0.611293   0.611293  0.611293  0.611293\n",
      "request                 0.615890   0.615890  0.615890  0.615890\n",
      "offer                   0.999015   0.999015  0.999015  0.999015\n",
      "aid_related             0.600788   0.600788  0.600788  0.600788\n",
      "medical_help            0.941563   0.941563  0.941563  0.941563\n",
      "medical_products        0.964544   0.964544  0.964544  0.964544\n",
      "search_and_rescue       0.979317   0.979317  0.979317  0.979317\n",
      "security                0.986868   0.986868  0.986868  0.986868\n",
      "military                0.996389   0.996389  0.996389  0.996389\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.918910   0.918910  0.918910  0.918910\n",
      "food                    0.843401   0.843401  0.843401  0.843401\n",
      "shelter                 0.885424   0.885424  0.885424  0.885424\n",
      "clothing                0.991464   0.991464  0.991464  0.991464\n",
      "money                   0.986540   0.986540  0.986540  0.986540\n",
      "missing_people          0.993434   0.993434  0.993434  0.993434\n",
      "refugees                0.983913   0.983913  0.983913  0.983913\n",
      "death                   0.971110   0.971110  0.971110  0.971110\n",
      "other_aid               0.838148   0.838148  0.838148  0.838148\n",
      "infrastructure_related  0.968155   0.968155  0.968155  0.968155\n",
      "transport               0.980959   0.980959  0.980959  0.980959\n",
      "buildings               0.960276   0.960276  0.960276  0.960276\n",
      "electricity             0.993106   0.993106  0.993106  0.993106\n",
      "tools                   0.996389   0.996389  0.996389  0.996389\n",
      "hospitals               0.995076   0.995076  0.995076  0.995076\n",
      "shops                   0.996060   0.996060  0.996060  0.996060\n",
      "aid_centers             0.992449   0.992449  0.992449  0.992449\n",
      "other_infrastructure    0.983585   0.983585  0.983585  0.983585\n",
      "weather_related         0.826658   0.826658  0.826658  0.826658\n",
      "floods                  0.970781   0.970781  0.970781  0.970781\n",
      "storm                   0.963887   0.963887  0.963887  0.963887\n",
      "fire                    0.994747   0.994747  0.994747  0.994747\n",
      "earthquake              0.907420   0.907420  0.907420  0.907420\n",
      "cold                    0.993762   0.993762  0.993762  0.993762\n",
      "other_weather           0.978332   0.978332  0.978332  0.978332\n",
      "direct_report           0.620486   0.620486  0.620486  0.620486\n"
     ]
    }
   ],
   "source": [
    "# Repeat evaluation for test set\n",
    "y_test_preds = pipeline.predict(X_test)\n",
    "\n",
    "print(get_performance_metrics(np.array(y_test), y_test_preds, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   0.680784\n",
       "request                   0.350832\n",
       "offer                     0.000985\n",
       "aid_related               0.384615\n",
       "medical_help              0.055747\n",
       "medical_products          0.033488\n",
       "search_and_rescue         0.020684\n",
       "security                  0.012804\n",
       "military                  0.004334\n",
       "child_alone               0.000000\n",
       "water                     0.077613\n",
       "food                      0.148232\n",
       "shelter                   0.104403\n",
       "clothing                  0.009061\n",
       "money                     0.012312\n",
       "missing_people            0.008372\n",
       "refugees                  0.016153\n",
       "death                     0.024328\n",
       "other_aid                 0.141928\n",
       "infrastructure_related    0.030040\n",
       "transport                 0.018812\n",
       "buildings                 0.036147\n",
       "electricity               0.006501\n",
       "tools                     0.002955\n",
       "hospitals                 0.005319\n",
       "shops                     0.002659\n",
       "aid_centers               0.007485\n",
       "other_infrastructure      0.016547\n",
       "weather_related           0.148232\n",
       "floods                    0.025116\n",
       "storm                     0.031813\n",
       "fire                      0.003841\n",
       "earthquake                0.082439\n",
       "cold                      0.005910\n",
       "other_weather             0.018517\n",
       "direct_report             0.339210\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how true target variable observations are split among Y categories predicted\n",
    "Y.sum()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see responses are highly imbalanced between categories predicted.  This makes it more difficult to predict categories with smaller positive class proportions.  It also means any positive predictions in our test set should be closely checked against the proportionality of actual members, as the performance could just be mirroring the proportionality rather than actually predicting correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'clf__estimator__max_depth': [5, 15]}#,\n",
    "            # 'clf__estimator__n_estimators': [5, 15],\n",
    "            # 'clf__estimator__max_features': [10, 15]}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters)\n",
    "\n",
    "np.random.seed(42)\n",
    "grid_search_model = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 16.65757871,  17.06446576]),\n",
       " 'std_fit_time': array([ 0.03911839,  0.04935915]),\n",
       " 'mean_score_time': array([ 8.02841457,  8.03691093]),\n",
       " 'std_score_time': array([ 0.0606768 ,  0.03099756]),\n",
       " 'param_clf__estimator__max_depth': masked_array(data = [5 15],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__max_depth': 5},\n",
       "  {'clf__estimator__max_depth': 15}],\n",
       " 'split0_test_score': array([ 0.18108907,  0.17729   ]),\n",
       " 'split1_test_score': array([ 0.17982271,  0.17686788]),\n",
       " 'split2_test_score': array([ 0.18657661,  0.18488814]),\n",
       " 'mean_test_score': array([ 0.18249613,  0.179682  ]),\n",
       " 'std_test_score': array([ 0.00293129,  0.00368532]),\n",
       " 'rank_test_score': array([1, 2], dtype=int32),\n",
       " 'split0_train_score': array([ 0.18341072,  0.18742085]),\n",
       " 'split1_train_score': array([ 0.18446602,  0.19164204]),\n",
       " 'split2_train_score': array([ 0.18087801,  0.18573238]),\n",
       " 'mean_train_score': array([ 0.18291825,  0.18826509]),\n",
       " 'std_train_score': array([ 0.00150562,  0.00248537])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See all grid search results\n",
    "grid_search_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__max_depth': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for best performing model\n",
    "grid_search_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall  F1 Score\n",
      "related                 0.661195   0.661195  0.661195  0.661195\n",
      "request                 0.648720   0.648720  0.648720  0.648720\n",
      "offer                   0.999015   0.999015  0.999015  0.999015\n",
      "aid_related             0.613920   0.613920  0.613920  0.613920\n",
      "medical_help            0.943533   0.943533  0.943533  0.943533\n",
      "medical_products        0.967170   0.967170  0.967170  0.967170\n",
      "search_and_rescue       0.979317   0.979317  0.979317  0.979317\n",
      "security                0.986868   0.986868  0.986868  0.986868\n",
      "military                0.996389   0.996389  0.996389  0.996389\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.926789   0.926789  0.926789  0.926789\n",
      "food                    0.853907   0.853907  0.853907  0.853907\n",
      "shelter                 0.894944   0.894944  0.894944  0.894944\n",
      "clothing                0.991464   0.991464  0.991464  0.991464\n",
      "money                   0.987196   0.987196  0.987196  0.987196\n",
      "missing_people          0.993434   0.993434  0.993434  0.993434\n",
      "refugees                0.984242   0.984242  0.984242  0.984242\n",
      "death                   0.972423   0.972423  0.972423  0.972423\n",
      "other_aid               0.858831   0.858831  0.858831  0.858831\n",
      "infrastructure_related  0.970781   0.970781  0.970781  0.970781\n",
      "transport               0.981615   0.981615  0.981615  0.981615\n",
      "buildings               0.962902   0.962902  0.962902  0.962902\n",
      "electricity             0.993434   0.993434  0.993434  0.993434\n",
      "tools                   0.996389   0.996389  0.996389  0.996389\n",
      "hospitals               0.995076   0.995076  0.995076  0.995076\n",
      "shops                   0.996060   0.996060  0.996060  0.996060\n",
      "aid_centers             0.992449   0.992449  0.992449  0.992449\n",
      "other_infrastructure    0.984242   0.984242  0.984242  0.984242\n",
      "weather_related         0.843401   0.843401  0.843401  0.843401\n",
      "floods                  0.975378   0.975378  0.975378  0.975378\n",
      "storm                   0.965857   0.965857  0.965857  0.965857\n",
      "fire                    0.994747   0.994747  0.994747  0.994747\n",
      "earthquake              0.913329   0.913329  0.913329  0.913329\n",
      "cold                    0.994091   0.994091  0.994091  0.994091\n",
      "other_weather           0.978989   0.978989  0.978989  0.978989\n",
      "direct_report           0.643795   0.643795  0.643795  0.643795\n"
     ]
    }
   ],
   "source": [
    "y_test_preds_tuned = grid_search_model.predict(X_test)\n",
    "\n",
    "print(get_performance_metrics(np.array(y_test), y_test_preds_tuned, col_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try XG Boost\n",
    "xg_pipeline = Pipeline([\n",
    "                        ('vector', CountVectorizer(tokenizer = tokenize)),\n",
    "                        ('tfidf', TfidfTransfomer()),\n",
    "                        ('xgb', MultiOutputClassifier(xgb.XGBClassifier()))\n",
    "                        ])\n",
    "\n",
    "xg_params = {'max_depth': [5, 10],\n",
    "             'n_estimators': [5, 10]}\n",
    "\n",
    "xg_cv = GridSearchCV(xg_pipeline, xg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize model\n",
    "np.random.seed(42)\n",
    "xg_model = xg_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate grid search model results\n",
    "xg_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find parameters that generated best performance\n",
    "xg_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_test_preds_xg = xg_model.predict(X_test)\n",
    "\n",
    "print(get_performance_metrics(np.array(y_test), y_test_preds, col_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_search_model, open('disaster_rf_clf_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
